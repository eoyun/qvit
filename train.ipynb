{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c7e5de5-f7a2-4e22-a3af-e8043edcb2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f263a536-ee5f-4b45-bd20-57805bb8c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "image_size = 98  # e.g., 98 (must be divisible by patch_size)\n",
    "patch_size = 7   # e.g., 7\n",
    "embed_dim = 16   # Transformer embedding dimension (must equal n_qubits_transformer for quantum attention)\n",
    "num_heads = 2\n",
    "num_blocks = 2\n",
    "ffn_dim = 32\n",
    "n_qubits_transformer = 16\n",
    "n_qubits_ffn = 0\n",
    "n_qlayers = 1\n",
    "q_device = \"default.qubit\"  # Quantum device (e.g., default.qubit, braket.qubit, etc.)\n",
    "\n",
    "dropout = 0.1\n",
    "epochs = 30\n",
    "batch_size = 8\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697b87e-02f6-4ab2-9b90-25f4e7425e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('rm_invalid.csv')\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df_mergedHard   = df[df['Label'] == 'mergedHard'].iloc[:20000]\n",
    "df_notMerged    = df[df['Label'] == 'notMerged'].iloc[:20000]\n",
    "df_notElectron  = df[df['Label'] == 'notElectron'].iloc[:20000]\n",
    "df_limited = pd.concat([df_mergedHard, df_notMerged, df_notElectron], ignore_index=True)\n",
    "df_limited = df_limited.sample(frac=1, random_state=42).reset_index(drop=True)  # shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e6af5-9392-4e5e-a13c-13a5e0eabdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare label mapping\n",
    "labels = sorted(df_limited['Label'].unique().tolist())  # sorted unique labels\n",
    "label_to_idx = {label: idx for idx, label in enumerate(labels)}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "num_classes = len(labels)\n",
    "print(\"Classes:\", labels)  # e.g., ['mergedHard', 'notElectron', 'notMerged']\n",
    "\n",
    "df_limited['LabelIdx'] = df_limited['Label'].map(label_to_idx)\n",
    "\n",
    "X = df_limited['ImagePath'].values\n",
    "y = df_limited['LabelIdx'].values\n",
    "# First split off 20% as test, then split the remaining 80% equally into train/val\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, \n",
    "    random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, \n",
    "    random_state=42, stratify=y_temp)\n",
    "print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}, Test size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a446be99-a78e-41d6-b2d3-10bbc30a8e76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Data transformations and augmentation\u001b[39;00m\n\u001b[32m      2\u001b[39m train_transforms = transforms.Compose([\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     transforms.Resize((\u001b[43mimage_size\u001b[49m, image_size), interpolation=transforms.InterpolationMode.BICUBIC),\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m## Random Zoom Out or In\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m#transforms.RandomApply([\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m#    transforms.RandomChoice([\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m#        transforms.RandomAffine(degrees=0, scale=(0.6, 0.9), fill=0),  # zoom out\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m#        transforms.RandomAffine(degrees=0, scale=(1.1, 1.4), fill=0)   # zoom in\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m#    ])\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m#], p=0.5),\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m## Random Rotation (±72° max)\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m#transforms.RandomApply([transforms.RandomRotation(degrees=72)], p=0.5),\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m## Random Brightness\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m#transforms.RandomApply([transforms.ColorJitter(brightness=0.2)], p=0.5),\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m## Random Contrast\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m#transforms.RandomApply([transforms.ColorJitter(contrast=0.2)], p=0.5),\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m## Random Shear (~20°)\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m#transforms.RandomApply([transforms.RandomAffine(degrees=0, shear=20, fill=0)], p=0.5),\u001b[39;00m\n\u001b[32m     19\u001b[39m     transforms.ToTensor(),\n\u001b[32m     20\u001b[39m     transforms.Normalize(mean=[\u001b[32m0.485\u001b[39m, \u001b[32m0.456\u001b[39m, \u001b[32m0.406\u001b[39m], std=[\u001b[32m0.229\u001b[39m, \u001b[32m0.224\u001b[39m, \u001b[32m0.225\u001b[39m])\n\u001b[32m     21\u001b[39m ])\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Transforms for validation/test (no augmentation)\u001b[39;00m\n\u001b[32m     24\u001b[39m test_transforms = transforms.Compose([\n\u001b[32m     25\u001b[39m     transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BICUBIC),\n\u001b[32m     26\u001b[39m     transforms.ToTensor(),\n\u001b[32m     27\u001b[39m     transforms.Normalize(mean=[\u001b[32m0.485\u001b[39m, \u001b[32m0.456\u001b[39m, \u001b[32m0.406\u001b[39m], std=[\u001b[32m0.229\u001b[39m, \u001b[32m0.224\u001b[39m, \u001b[32m0.225\u001b[39m])\n\u001b[32m     28\u001b[39m ])\n",
      "\u001b[31mNameError\u001b[39m: name 'image_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Data transformations and augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    ## Random Zoom Out or In\n",
    "    #transforms.RandomApply([\n",
    "    #    transforms.RandomChoice([\n",
    "    #        transforms.RandomAffine(degrees=0, scale=(0.6, 0.9), fill=0),  # zoom out\n",
    "    #        transforms.RandomAffine(degrees=0, scale=(1.1, 1.4), fill=0)   # zoom in\n",
    "    #    ])\n",
    "    #], p=0.5),\n",
    "    ## Random Rotation (±72° max)\n",
    "    #transforms.RandomApply([transforms.RandomRotation(degrees=72)], p=0.5),\n",
    "    ## Random Brightness\n",
    "    #transforms.RandomApply([transforms.ColorJitter(brightness=0.2)], p=0.5),\n",
    "    ## Random Contrast\n",
    "    #transforms.RandomApply([transforms.ColorJitter(contrast=0.2)], p=0.5),\n",
    "    ## Random Shear (~20°)\n",
    "    #transforms.RandomApply([transforms.RandomAffine(degrees=0, shear=20, fill=0)], p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transforms for validation/test (no augmentation)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36453dac-bfd5-4e93-a808-445fe3165f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels  # numeric labels\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Open image (ensure 3 channels)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = ImageDataset(X_train, y_train, transform=train_transforms)\n",
    "val_dataset   = ImageDataset(X_val,   y_val,   transform=test_transforms)\n",
    "test_dataset  = ImageDataset(X_test,  y_test,  transform=test_transforms)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size*2, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee19148-bb8e-4da7-8b73-37e5e391df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from qvit import VisionTransformer\n",
    "\n",
    "# Instantiate the Vision Transformer model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionTransformer(\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    in_channels=3,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_blocks=num_blocks,\n",
    "    num_classes=num_classes,\n",
    "    ffn_dim=ffn_dim,\n",
    "    n_qubits_transformer=n_qubits_transformer,\n",
    "    n_qubits_ffn=n_qubits_ffn,\n",
    "    n_qlayers=n_qlayers,\n",
    "    dropout=dropout,\n",
    "    q_device=q_device\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bef9bd-5739-4a3a-88dd-ba6f699f8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Define focal loss function\n",
    "def focal_loss(inputs, targets, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Compute the focal loss between `inputs` (logits) and `targets` (integer class indices).\n",
    "    \"\"\"\n",
    "    # Standard cross-entropy (not averaged, per-sample)\n",
    "    ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "    # p_t: probability of the true class for each sample\n",
    "    p_t = torch.exp(-ce_loss)\n",
    "    # Focal loss computation\n",
    "    loss = alpha * ((1 - p_t) ** gamma) * ce_loss\n",
    "    if reduction == 'mean':\n",
    "        return loss.mean()\n",
    "    elif reduction == 'sum':\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "# Instantiate optimizer (AdamW)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler: ReduceLROnPlateau (monitor val F1)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.8, patience=3)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_val_f1 = -float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Mixed precision setup\n",
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65eec2-bdf2-462f-bd69-97fa64809199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "metrics_history = {\n",
    "    \"train_loss\": [], \"val_loss\": [],\n",
    "    \"train_f1\": [], \"val_f1\": []\n",
    "}\n",
    "best_model_path = f\"./ckpts/pytorch/wo_meta_{pd.Timestamp.now():%Y%m%d_%H%M}/best_model.pth\"\n",
    "os.makedirs(os.path.dirname(best_model_path), exist_ok=True)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    all_train_preds = []\n",
    "    all_train_labels = []\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Mixed precision forward and loss\n",
    "        with autocast():\n",
    "            logits = model(images)  # model outputs logits directly\n",
    "            loss = focal_loss(logits, labels)\n",
    "        train_losses.append(loss.item())\n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # Collect predictions for F1\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_train_preds.append(preds.detach().cpu().numpy())\n",
    "        all_train_labels.append(labels.detach().cpu().numpy())\n",
    "    # Compute train loss and F1\n",
    "    train_loss = np.mean(train_losses)\n",
    "    all_train_preds = np.concatenate(all_train_preds)\n",
    "    all_train_labels = np.concatenate(all_train_labels)\n",
    "    train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with autocast():\n",
    "                logits = model(images)\n",
    "                loss = focal_loss(logits, labels)\n",
    "            val_losses.append(loss.item())\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_val_preds.append(preds.cpu().numpy())\n",
    "            all_val_labels.append(labels.cpu().numpy())\n",
    "    val_loss = np.mean(val_losses)\n",
    "    all_val_preds = np.concatenate(all_val_preds)\n",
    "    all_val_labels = np.concatenate(all_val_labels)\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')\n",
    "    \n",
    "    # Record metrics\n",
    "    metrics_history[\"train_loss\"].append(train_loss)\n",
    "    metrics_history[\"val_loss\"].append(val_loss)\n",
    "    metrics_history[\"train_f1\"].append(train_f1)\n",
    "    metrics_history[\"val_f1\"].append(val_f1)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Epoch {epoch:03d}: Train Loss={train_loss:.4f}, Train F1={train_f1:.4f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val F1={val_f1:.4f}\")\n",
    "    \n",
    "    # LR scheduling step (monitor val F1)\n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        epochs_no_improve = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        best_epoch = epoch\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"No improvement for {patience} epochs. Early stopping at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "print(f\"Best model was from epoch {best_epoch} with Val F1 = {best_val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d5596-9978-4b6e-a3d1-3967b9bea117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Load the best model for testing\n",
    "best_model = VisionTransformer(\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    in_channels=3,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_blocks=num_blocks,\n",
    "    num_classes=num_classes,\n",
    "    ffn_dim=ffn_dim,\n",
    "    n_qubits_transformer=n_qubits_transformer,\n",
    "    n_qubits_ffn=n_qubits_ffn,\n",
    "    n_qlayers=n_qlayers,\n",
    "    dropout=dropout,\n",
    "    q_device=q_device\n",
    ")\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.to(device)\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d312b-46ab-4c29-a272-318f57148756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set\n",
    "y_true = []\n",
    "y_prob = []  # probabilities for each class\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = best_model(images)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        y_true.extend(labels.cpu().numpy().tolist())\n",
    "        y_prob.extend(probs.cpu().numpy().tolist())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_prob = np.array(y_prob)  # shape (n_samples, num_classes)\n",
    "\n",
    "# Determine predicted class indices\n",
    "y_pred_indices = np.argmax(y_prob, axis=1)\n",
    "# Map indices to label names\n",
    "y_pred_labels = [idx_to_label[idx] for idx in y_pred_indices]\n",
    "y_true_labels = [idx_to_label[idx] for idx in y_true]\n",
    "\n",
    "# Save predictions to CSV\n",
    "results_dir = f\"./results/pytorch/wo_meta_{pd.Timestamp.now():%Y%m%d_%H%M}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "df_submission = pd.DataFrame({\"pred\": y_pred_labels, \"true\": y_true_labels})\n",
    "df_submission.to_csv(os.path.join(results_dir, \"predictions.csv\"), index=False)\n",
    "print(\"Saved test predictions to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec565487-6da5-4fd8-bb67-54923657e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history (Loss and F1 over epochs)\n",
    "epochs_range = range(1, len(metrics_history[\"train_loss\"]) + 1)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs_range, metrics_history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(epochs_range, metrics_history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss vs Epochs\"); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs_range, metrics_history[\"train_f1\"], label=\"Train F1\")\n",
    "plt.plot(epochs_range, metrics_history[\"val_f1\"], label=\"Val F1\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"F1 Score\"); plt.title(\"F1 Score vs Epochs\"); plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"training_history.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d7588-30c1-4276-b3b4-b4cd0337ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Check shapes\n",
    "assert y_prob.ndim == 2, f\"y_prob shape weird: {y_prob.shape}\"\n",
    "assert len(y_true) == y_prob.shape[0], f\"len(y_true)={len(y_true)} vs y_prob={y_prob.shape}\"\n",
    "\n",
    "# 2) Class names\n",
    "num_classes = y_prob.shape[1]\n",
    "class_indices = list(range(num_classes))\n",
    "try:\n",
    "    class_names = [idx_to_label[i] for i in class_indices]\n",
    "except Exception:\n",
    "    class_names = [f\"class_{i}\" for i in class_indices]\n",
    "\n",
    "# 3) Binarize\n",
    "y_true_bin = label_binarize(y_true, classes=class_indices)  # shape: (N, num_classes)\n",
    "\n",
    "# 4) Plot ROC (skip if one class)\n",
    "plt.figure(figsize=(6, 6))\n",
    "any_plotted = False\n",
    "for i, name in enumerate(class_names):\n",
    "    y_true_i = y_true_bin[:, i]\n",
    "    # Skip if only one class in true labels\n",
    "    if y_true_i.max() == 0 or y_true_i.min() == 1:\n",
    "        print(f\"[ROC] Skip '{name}': only one class present in y_true for this label.\")\n",
    "        continue\n",
    "    fpr, tpr, _ = roc_curve(y_true_i, y_prob[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
    "    any_plotted = True\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Multi-Class ROC Curve\")\n",
    "if any_plotted:\n",
    "    plt.legend(loc=\"lower right\")\n",
    "else:\n",
    "    plt.legend([], frameon=False)\n",
    "plt.savefig(os.path.join(results_dir, \"roc_curve.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87966096-4456-41f5-acd8-652c61012282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cml-conda",
   "language": "python",
   "name": "cml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
